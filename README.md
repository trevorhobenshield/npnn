# npnn

### `mlp.py`
Simple MLP in NumPy using mini-batch SGD with L2 reg. and Dropout

`f`,`df`: activation function and it's derivative

`J`,`dJ`: loss function and it's derivative

`gW`,`gb`: gradients w.r.t. model parameters

