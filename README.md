# npnn

### `mlp.py`
Simple MLP in NumPy using mini-batch SGD with L2 Regularization and Dropout.

`f`,`df`: activation function and it's derivative

`C`,`dC`: loss function and it's derivative

`gW`,`gb`: gradients w.r.t. the model's parameters

